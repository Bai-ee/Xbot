const { BaseAgent } = require('./BaseAgent.js');
const openaiClient = require('../api/openaiClient.js');
const path = require('path');
const fs = require('fs-extra');

// Import the enhanced video processing libraries
const { ArweaveVideoGenerator } = require('../lib/ArweaveVideoGenerator.js');
const { ArweaveAudioClient } = require('../lib/ArweaveAudioClient.js');

class ArweaveVideoAgent extends BaseAgent {
  constructor() {
    super('ArweaveVideoAgent', ['video_generation', 'arweave_audio', 'content_creation']);
    
    try {
      this.videoGenerator = new ArweaveVideoGenerator();
      this.audioClient = new ArweaveAudioClient();
      console.log('✅ ArweaveVideoAgent initialized with ArweaveVideoGenerator');
    } catch (error) {
      console.error('❌ Failed to initialize ArweaveVideoAgent:', error);
      this.videoGenerator = null;
      this.audioClient = null;
    }
  }

  async handleMessage(input, context = {}) {
    console.log('🎬 ArweaveVideoAgent processing request:', input.substring(0, 100) + '...');

    if (!this.videoGenerator) {
      throw new Error('ArweaveVideoGenerator not initialized');
    }

    try {
      // Check if audio was already generated by audio_generator agent
      const existingAudio = this.checkForExistingAudio(context);
      
      // Parse request parameters
      const duration = this.extractDuration(input, context);
      const artist = this.extractArtist(input, context);

      console.log(`🎯 Video generation request: ${duration}s${artist ? ` for ${artist}` : ' (random)'}`);
      
      if (existingAudio) {
        console.log(`🎵 Using existing audio from audio_generator: ${existingAudio.audioFileName}`);
      }

      // Generate video with real Arweave audio (or use existing audio)
      const videoResult = await this.videoGenerator.generateVideoWithAudio({
        duration,
        artist,
        prompt: input,
        width: context.width || 720,
        height: context.height || 720,
        fadeIn: context.fadeIn || 2,
        fadeOut: context.fadeOut || 2,
        existingAudio: existingAudio // Pass existing audio if available
      });

      if (videoResult.success) {
        console.log(`✅ Video generated: ${videoResult.artist} - ${videoResult.mixTitle}`);
        
        return {
          type: 'video_generation',
          success: true,
          video: videoResult,
          workflow: 'arweave_video_generation',
          metadata: videoResult.metadata,
          message: `Generated ${videoResult.duration}s video for ${videoResult.artist} with real Arweave audio`
        };
      } else {
        return {
          type: 'video_generation',
          success: false,
          error: videoResult.error,
          message: `Video generation failed: ${videoResult.error}`
        };
      }
      
    } catch (error) {
      console.error('❌ ArweaveVideoAgent execution failed:', error);
      
      return {
        type: 'video_generation', 
        success: false,
        error: error.message,
        message: `Video generation failed: ${error.message}`
      };
    }
  }

  /**
   * Extract duration from input or context
   */
  extractDuration(input, context) {
    if (context.duration) {
      return parseInt(context.duration);
    }

    if (this.audioClient) {
      return this.audioClient.extractRequestedDuration(input) || 30;
    }

    return 30;
  }

  /**
   * Check for existing audio generated by audio_generator agent
   */
  checkForExistingAudio(context) {
    // Check if audio was already generated by audio_generator agent
    if (context.audioPath && context.audioFileName) {
      return {
        audioPath: context.audioPath,
        audioUrl: context.audioUrl,
        audioFileName: context.audioFileName,
        audioArtist: context.audioArtist,
        audioMixTitle: context.audioMixTitle,
        audioDuration: context.audioDuration,
        audioFileSize: context.audioFileSize,
        audioArweaveUrl: context.audioArweaveUrl,
        audioMetadata: context.audioMetadata,
        audioType: context.audioType
      };
    }
    
    // Also check in previousResults if this is a sequential workflow
    if (context.previousResults && Array.isArray(context.previousResults)) {
      for (const result of context.previousResults) {
        if (result.type === 'audio_generation' && result.success && result.audioPath) {
          return {
            audioPath: result.audioPath,
            audioUrl: result.url,
            audioFileName: result.fileName,
            audioArtist: result.artist,
            audioMixTitle: result.mixTitle,
            audioDuration: result.duration,
            audioFileSize: result.fileSize,
            audioArweaveUrl: result.arweaveUrl,
            audioMetadata: result.metadata,
            audioType: result.type
          };
        }
      }
    }
    
    return null;
  }

  /**
   * Extract artist from input or context
   */
  extractArtist(input, context) {
    if (context.artist && context.artist !== 'random') {
      return context.artist;
    }

    if (this.audioClient) {
      return this.audioClient.extractArtistFromPrompt(input);
    }

    return null;
  }

  async analyzeVideoRequest(input, context) {
    const prompt = `Analyze this video generation request and extract key information:

Request: "${input}"

Extract and return JSON with:
- artist: specific artist name mentioned or "random" 
- duration: video length in seconds (default 30)
- style: video style preference ("classic", "modern", "minimal", etc.)
- audioSource: "arweave" or "upload" 
- customContent: any specific content to include
- urgency: "high", "medium", "low"

Example: {"artist": "random", "duration": 30, "style": "classic", "audioSource": "arweave", "customContent": null, "urgency": "medium"}`;

    try {
      const analysis = await openaiClient.runCompletion(prompt, null, {
        temperature: 0.3,
        max_tokens: 200
      });

      // Try to parse JSON response
      try {
        return JSON.parse(analysis.response);
      } catch (parseError) {
        // Fallback to defaults if JSON parsing fails
        return {
          artist: "random",
          duration: 30,
          style: "classic", 
          audioSource: "arweave",
          customContent: input,
          urgency: "medium"
        };
      }
    } catch (error) {
      console.error('❌ Failed to analyze video request:', error.message);
      // Return default video request
      return {
        artist: "random",
        duration: 30,
        style: "classic",
        audioSource: "arweave", 
        customContent: input,
        urgency: "medium"
      };
    }
  }

  determineVideoWorkflow(videoRequest, context) {
    const steps = [];
    
    // Always include core steps
    steps.push('load_artist_data');
    
    // Add audio processing step
    if (videoRequest.audioSource === 'arweave') {
      steps.push('fetch_arweave_audio');
    } else {
      steps.push('process_uploaded_audio');
    }
    
    // Add visual generation steps
    steps.push('generate_visuals');
    steps.push('create_layout');
    
    // Add video composition
    steps.push('compose_video');
    
    // Optional steps based on request
    if (videoRequest.style === 'enhanced' || context.includeAI) {
      steps.push('ai_background_generation');
    }
    
    if (videoRequest.urgency === 'low') {
      steps.push('quality_optimization');
    }

    // Determine workflow type
    let workflowType = 'simple';
    if (steps.length > 5) {
      workflowType = 'comprehensive';
    } else if (steps.length > 3) {
      workflowType = 'moderate';
    }

    return {
      type: workflowType,
      steps,
      canSkip: ['ai_background_generation', 'quality_optimization'],
      canAdd: ['social_media_optimization', 'multiple_formats']
    };
  }

  async executeVideoWorkflow(workflow, videoRequest, context) {
    console.log(`🔄 Executing ${workflow.type} video workflow with ${workflow.steps.length} steps`);
    
    const results = {};
    let currentData = { ...videoRequest };

    try {
      for (const step of workflow.steps) {
        console.log(`📋 Executing step: ${step}`);
        
        switch (step) {
          case 'load_artist_data':
            currentData.artistData = await this.loadArtistData(currentData.artist);
            break;
            
          case 'fetch_arweave_audio':
            currentData.audioData = await this.fetchArweaveAudio(currentData.artistData);
            break;
            
          case 'process_uploaded_audio':
            currentData.audioData = await this.processUploadedAudio(context.uploadedFiles);
            break;
            
          case 'generate_visuals':
            currentData.visuals = await this.generateVisuals(currentData);
            break;
            
          case 'create_layout':
            currentData.layout = await this.createVideoLayout(currentData);
            break;
            
          case 'ai_background_generation':
            if (!workflow.canSkip.includes(step) || context.forceAI) {
              currentData.aiBackground = await this.generateAIBackground(currentData);
            }
            break;
            
          case 'compose_video':
            currentData.video = await this.composeVideo(currentData);
            break;
            
          case 'quality_optimization':
            if (!workflow.canSkip.includes(step)) {
              currentData.video = await this.optimizeVideo(currentData.video);
            }
            break;
        }
        
        results[step] = { success: true, timestamp: new Date().toISOString() };
      }

      return {
        success: true,
        workflow: workflow.type,
        artist: currentData.artistData?.artistName || 'Unknown',
        duration: currentData.duration,
        videoPath: currentData.video?.path || null,
        steps: results,
        metadata: currentData
      };
      
    } catch (error) {
      console.error('❌ Video workflow execution failed:', error.message);
      
      return {
        success: false,
        error: error.message,
        workflow: workflow.type,
        completedSteps: Object.keys(results),
        failedStep: workflow.steps.find(step => !results[step])
      };
    }
  }

  async loadArtistData(artistName) {
    // Load from sample artists data
    const artistsPath = path.join(process.cwd(), 'data', 'sample-artists.json');
    
    try {
      if (await fs.pathExists(artistsPath)) {
        const artists = await fs.readJson(artistsPath);
        
        if (artistName === 'random') {
          return artists[Math.floor(Math.random() * artists.length)];
        } else {
          const found = artists.find(a => 
            a.artistName.toLowerCase().includes(artistName.toLowerCase())
          );
          return found || artists[0]; // Fallback to first artist
        }
      }
    } catch (error) {
      console.warn('⚠️ Could not load artists data:', error.message);
    }

    // Fallback artist data
    return {
      artistName: "Sample Artist",
      artistGenre: "electronic",
      mixes: [{
        mixTitle: "Hello World Mix",
        mixArweaveURL: "https://arweave.net/sample-audio-hash",
        mixDuration: "30:00",
        mixDateYear: "'24"
      }]
    };
  }

  async fetchArweaveAudio(artistData) {
    console.log('🎵 Processing Arweave audio for:', artistData.artistName);
    
    const selectedMix = artistData.mixes[Math.floor(Math.random() * artistData.mixes.length)];
    
    try {
      // Use real ArweaveAudioProcessor to download and process audio
      const audioResult = await this.audioProcessor.processArweaveAudio(
        selectedMix.mixArweaveURL,
        {
          duration: 30,
          fadeIn: 2,
          fadeOut: 2,
          quality: 'high'
        }
      );

      return {
        title: selectedMix.mixTitle,
        url: selectedMix.mixArweaveURL,
        duration: audioResult.clippedDuration,
        format: 'mp3',
        localPath: audioResult.audioPath,
        startTime: audioResult.startTime,
        metadata: audioResult.metadata,
        mock: false // Real implementation
      };

    } catch (error) {
      console.warn('⚠️ Arweave audio processing failed, using fallback:', error.message);
      
      // Fallback to mock data if real processing fails
      return {
        title: selectedMix.mixTitle,
        url: selectedMix.mixArweaveURL,
        duration: 30,
        format: 'mp3',
        localPath: null,
        mock: true,
        error: error.message
      };
    }
  }

  async processUploadedAudio(uploadedFiles) {
    // Process user-uploaded audio files
    if (!uploadedFiles || uploadedFiles.length === 0) {
      throw new Error('No audio files uploaded');
    }
    
    const audioFile = uploadedFiles.find(f => f.mimetype.startsWith('audio/'));
    if (!audioFile) {
      throw new Error('No valid audio files found');
    }

    try {
      // Use real audio processor for uploaded files
      const audioResult = await this.audioProcessor.processUploadedAudio(
        audioFile.path,
        {
          duration: 30,
          fadeIn: 2,
          fadeOut: 2,
          quality: 'high'
        }
      );

      return {
        title: audioFile.originalname,
        localPath: audioResult.audioPath,
        duration: audioResult.clippedDuration,
        format: audioFile.mimetype.split('/')[1],
        startTime: audioResult.startTime,
        metadata: audioResult.metadata,
        mock: false
      };

    } catch (error) {
      console.warn('⚠️ Uploaded audio processing failed:', error.message);
      throw new Error(`Failed to process uploaded audio: ${error.message}`);
    }
  }

  async generateVisuals(data) {
    console.log('🎨 Generating visuals for:', data.artistData.artistName);
    
    // Simple visual generation - in real implementation this would create actual images
    const colors = ['#667eea', '#764ba2', '#f093fb', '#f5576c', '#4facfe'];
    const selectedColor = colors[Math.floor(Math.random() * colors.length)];
    
    return {
      backgroundColor: selectedColor,
      artistImage: `/assets/artists/${data.artistData.artistName.toLowerCase().replace(/\s+/g, '-')}.jpg`,
      logoImage: '/assets/logos/sample-logo.png',
      style: data.style || 'classic',
      dimensions: { width: 1080, height: 1080 }
    };
  }

  async createVideoLayout(data) {
    console.log('📐 Creating enhanced video layout');
    
    try {
      // Use enhanced HTML renderer to generate layout
      const htmlContent = this.htmlRenderer.generateEnhancedLayout(data);
      
      const layoutId = Date.now();
      const layoutPath = path.join(this.tempDir, `layout-${layoutId}.html`);
      await fs.writeFile(layoutPath, htmlContent);

      console.log('✅ Enhanced HTML layout created');

      return {
        htmlPath: layoutPath,
        dimensions: data.visuals.dimensions,
        style: data.visuals.style,
        enhanced: true
      };

    } catch (error) {
      console.warn('⚠️ Enhanced layout creation failed, using simple layout:', error.message);
      
      // Fallback to simple HTML layout
      const simpleHtmlContent = `
      <!DOCTYPE html>
      <html>
      <head>
          <style>
              body {
                  margin: 0;
                  padding: 0;
                  width: 1080px;
                  height: 1080px;
                  background: linear-gradient(45deg, ${data.visuals.backgroundColor}, #000);
                  display: flex;
                  flex-direction: column;
                  justify-content: center;
                  align-items: center;
                  font-family: 'Arial', sans-serif;
                  color: white;
              }
              .artist-name {
                  font-size: 48px;
                  font-weight: bold;
                  margin-bottom: 20px;
                  text-shadow: 2px 2px 4px rgba(0,0,0,0.5);
              }
              .mix-title {
                  font-size: 32px;
                  margin-bottom: 40px;
                  opacity: 0.9;
              }
              .logo {
                  position: absolute;
                  bottom: 40px;
                  right: 40px;
                  font-size: 24px;
                  opacity: 0.8;
              }
          </style>
      </head>
      <body>
          <div class="artist-name">${data.artistData.artistName}</div>
          <div class="mix-title">${data.audioData.title}</div>
          <div class="logo">CreativeTech DJ</div>
      </body>
      </html>`;

      const layoutPath = path.join(this.tempDir, `simple-layout-${Date.now()}.html`);
      await fs.writeFile(layoutPath, simpleHtmlContent);

      return {
        htmlPath: layoutPath,
        dimensions: data.visuals.dimensions,
        style: 'simple_fallback',
        enhanced: false
      };
    }
  }

  async generateAIBackground(data) {
    console.log('🤖 Generating AI background with DALL-E');
    
    try {
      // Use real AI background generator
      const aiBackground = await this.aiBackgroundGenerator.generateBackground({
        artistName: data.artistData.artistName,
        genre: data.artistData.artistGenre || 'electronic',
        style: data.style === 'enhanced' ? 'abstract' : 'minimal',
        mood: 'energetic',
        colors: [data.visuals.backgroundColor || '#667eea', '#000000'],
        width: 1080,
        height: 1080,
        quality: 'hd'
      });

      return {
        prompt: aiBackground.prompt,
        imagePath: aiBackground.imagePath,
        style: 'ai_generated',
        width: aiBackground.width,
        height: aiBackground.height,
        metadata: aiBackground.metadata,
        mock: false
      };

    } catch (error) {
      console.warn('⚠️ AI background generation failed, using fallback:', error.message);
      
      // Fallback to simple gradient
      return {
        prompt: `Fallback gradient background for ${data.artistData.artistName}`,
        imagePath: null,
        style: 'gradient_fallback',
        mock: true,
        error: error.message
      };
    }
  }

  async composeVideo(data) {
    console.log('🎬 Creating video with SimpleVideoGenerator');
    
    try {
      // Use the simple video generator with artist data
      const videoResult = await this.simpleVideoGenerator.generateSimpleVideo(
        {
          name: data.artistData.artistName,
          genre: data.artistData.artistGenre,
          mixCount: data.artistData.mixes ? data.artistData.mixes.length : 1
        },
        {
          duration: data.duration || 30,
          width: 720,
          height: 720
        }
      );

      console.log('✅ Simple video generation completed:', videoResult.filename);
      
      return {
        path: videoResult.path,
        url: videoResult.url,
        filename: videoResult.filename,
        duration: videoResult.duration,
        format: 'mp4',
        resolution: videoResult.size,
        fileSize: videoResult.fileSize,
        artist: videoResult.artist,
        created: new Date().toISOString(),
        mock: videoResult.mock || false
      };

    } catch (error) {
      console.error('❌ Simple video generation failed:', error.message);
      
      // Create mock response so UI shows something
      const mockFilename = `mock_${data.artistData.artistName.replace(/[^a-zA-Z0-9]/g, '_')}_${Date.now()}.mp4`;
      
      return {
        path: null,
        url: null,
        filename: mockFilename,
        duration: data.duration || 30,
        format: 'mp4',
        resolution: '720x720',
        fileSize: '2.1MB',
        artist: data.artistData.artistName,
        created: new Date().toISOString(),
        mock: true,
        error: error.message
      };
    }
  }

  async createSimpleVideoFromImage(imagePath, options) {
    // Create a simple silent video from a single image
    const { outputFilename, duration, metadata } = options;
    
    // Generate silent audio for the duration
    const silentAudioPath = await this.generateSilentAudio(duration);
    
    try {
      const videoResult = await this.videoCompositor.createVideoFromImageAndAudio({
        imagePath,
        audioPath: silentAudioPath,
        outputFilename,
        duration,
        quality: 'high',
        metadata
      });

      // Cleanup silent audio
      await fs.remove(silentAudioPath);

      return videoResult;

    } catch (error) {
      await fs.remove(silentAudioPath);
      throw error;
    }
  }

  async generateSilentAudio(duration) {
    // Use FFmpeg to generate silent audio
    const ffmpeg = require('fluent-ffmpeg');
    const ffmpegPath = require('ffmpeg-static');
    ffmpeg.setFfmpegPath(ffmpegPath);

    const silentAudioPath = path.join(this.tempDir, `silent-${Date.now()}.mp3`);

    return new Promise((resolve, reject) => {
      ffmpeg()
        .input('anullsrc=channel_layout=stereo:sample_rate=44100')
        .inputFormat('lavfi')
        .duration(duration)
        .audioCodec('mp3')
        .output(silentAudioPath)
        .on('end', () => resolve(silentAudioPath))
        .on('error', reject)
        .run();
    });
  }

  getAnimationsForStyle(style) {
    // Return animations based on style
    const animations = [
      {
        selector: '.artist-name',
        type: 'fadeIn',
        duration: 2
      },
      {
        selector: '.mix-title',
        type: 'slideIn',
        duration: 1.5,
        distance: 50
      }
    ];

    if (style === 'enhanced') {
      animations.push({
        selector: '.logo',
        type: 'pulse',
        speed: 2,
        amplitude: 0.1
      });
    }

    return animations;
  }

  async optimizeVideo(videoData) {
    console.log('⚡ Optimizing video quality');
    
    // Mock optimization - real implementation would re-encode with better settings
    return {
      ...videoData,
      optimized: true,
      bitrate: '2000k',
      codec: 'h264'
    };
  }

  // Utility methods
  async cleanup() {
    console.log('🧹 Starting comprehensive cleanup...');
    
    try {
      // Clean up temporary files
      const tempFiles = await fs.readdir(this.tempDir);
      const oldFiles = tempFiles.filter(file => {
        const filePath = path.join(this.tempDir, file);
        const stats = fs.statSync(filePath);
        const age = Date.now() - stats.mtime.getTime();
        return age > 24 * 60 * 60 * 1000; // Older than 24 hours
      });

      for (const file of oldFiles) {
        await fs.remove(path.join(this.tempDir, file));
      }

      console.log(`🗑️ Cleaned up ${oldFiles.length} temporary files`);

          // Cleanup video generator
    if (this.videoGenerator) {
      await this.videoGenerator.cleanup();
    }

      console.log('✅ Comprehensive cleanup completed');

    } catch (error) {
      console.warn('⚠️ Cleanup failed:', error.message);
    }
  }

  async closeResources() {
    // Close browser and other resources
    try {
      await this.htmlRenderer.close();
      console.log('🔒 Video generation resources closed');
    } catch (error) {
      console.warn('⚠️ Error closing resources:', error.message);
    }
  }

  getVideoHistory() {
    // Return list of generated videos
    try {
      const videos = fs.readdirSync(this.outputDir)
        .filter(file => file.endsWith('.mp4'))
        .map(file => ({
          filename: file,
          path: path.join(this.outputDir, file),
          created: fs.statSync(path.join(this.outputDir, file)).mtime
        }))
        .sort((a, b) => b.created - a.created);

      return videos;
    } catch (error) {
      console.error('❌ Error getting video history:', error.message);
      return [];
    }
  }
}

module.exports = { ArweaveVideoAgent }; 